**第一章 什么是Spark*


本章内容：
- 什么是 Spark 及其使用场景
- 分布式技术的基础
- Spark的四大支柱
- 存储及 APIs：爱上数据框架

上世纪80年代，当我还是个孩子的时候，通过 Basic 和我的雅达利（Atari）编程，我无法理解为什么我们不能实现基本执法活动的自动化，比如速度控制、闯红灯和停车计时器。一切似乎都很简单:我在书中说过，要成为一名优秀的程序员，应该避免 `GOTO` 语句。这就是我所做的，从 12 岁开始，我就试图构造我的代码。但是，在开发类似《大富翁》的游戏时，我无法想象数据量（以及蓬勃发展的物联网或物联网）。 当我的游戏适合64 KB的内存时，我绝对不知道数据集会变得更大（由一个巨大因素决定），或者数据会具有速度或速率，因为我耐心地等待着保存游戏到我的 Atari 1010 录音机。


短短35年之后，所有我想象中的用例似乎都是可访问的(而我的游戏则是无用的)。数据的增长速度已经超过了支持数据的硬件技术。一组小型计算机的成本比一台大型计算机还要低。内存比2005年便宜了一半，而2005年的内存比2000年便宜了五倍。网络速度要快很多倍，现代数据中心提供高达100Gbps的速度，比五年前的家庭Wi-Fi快近2000倍。这就是促使人们问这个问题的一些因素:我如何使用分布式内存计算来分析大量数据。

当您阅读文献或在web上搜索关于Apache Spark的信息时，您可能会发现它是一个用于大数据的工具、Hadoop的继承者、分析平台、集群计算机框架等等。


本章试验可以在这里找到：https://github.com/jgperrin/net.jgp.books.spark.ch01

> iooffqqx <> sharklasers.com 123456
>
> https://livebook.manning.com/book/spark-in-action-second-edition/chapter-1/48

# Spark是什么以及能做些什么
## Spark是什么
Spark不仅仅是一个数据科学家的软件栈。当您构建应用程序时，您是在操作系统之上构建它们的。操作系统提供的服务使您的应用程序开发更加轻松； 换句话说，您并不是在为开发的每个应用程序构建文件系统或网络驱动程序。

![](https://drek4537l1klr.cloudfront.net/perrin/HighResolutionFigures/figure_1-1.png)

随着对更多计算能力的需求，对分布式计算的需求也越来越大。 随着分布式计算的出现，分布式应用程序必须合并那些分布功能。下图展示了向你的应用添加组件所增加的复杂性。

![](https://drek4537l1klr.cloudfront.net/perrin/HighResolutionFigures/figure_1-2.png)


综上所述，Apache Spark可能看起来像一个复杂的系统，需要您具有很多先验知识。 我坚信您只需要Java和关系数据库管理系统（RDBMS）技能即可理解，使用，构建带有Spark的应用程序以及扩展Spark。

应用程序也变得更加智能，可以生成报告并执行数据分析（包括数据聚合，线性回归或仅显示甜甜圈图）。 因此，当您想向应用程序中添加此类分析功能时，必须链接库或构建自己的库。 所有这些使您的应用程序变得更大（或在胖客户端中变得更胖），更难维护，更复杂，因此对企业而言更加昂贵。

那么为什么不把这些功能放在操作系统级别呢?你可能会问。将这些特性放在较低的级别(如操作系统)有很多好处，包括以下方面：
- 提供一种处理数据的标准方法（有点像关系数据库的结构化查询语言或SQL）。
- 降低应用程序的开发（和维护）成本。
- 使您可以专注于理解如何使用该工具，而不是该工具的工作方式。 （例如，Spark执行分布式提取，您可以学习如何从中受益，而不必完全掌握Spark完成任务的方式。）

对我来说，Spark已经变成了一个分析操作系统。图1.3显示了这个简化的堆栈。

**图1.3 Apache Spark通过向应用程序提供服务，简化了面向分析的应用程序的开发，就像操作系统一样。**

![](https://drek4537l1klr.cloudfront.net/perrin/HighResolutionFigures/figure_1-3.png)

在本章中，您将发现针对不同行业和不同项目规模的Apache Spark的一些用例。 这些示例将简要概述您可以实现的目标。

我坚信，为了更好地了解我们的位置，我们应该回顾历史。 这也适用于信息技术（IT）：如果需要，请阅读[附录E](appendix/E.md)。

现在已经设置好场景，您将深入研究Spark。 我们将从全局概述开始，看一下存储和API，最后，完成第一个示例。

## 四大支柱
根据波利尼西亚人的说法，法力是体现在物体或人体内的自然基本力量的力量。 此定义适合您在所有Spark文档中都能找到的经典图表，显示了将这些基本要素带入Spark的四个支柱：Spark SQL，Spark Streaming，Spark MLlib（用于机器学习）和位于Spark Core上方的GraphX。 尽管这是Spark堆栈的精确表示，但我发现它是有限制的。 需要扩展堆栈以显示硬件，操作系统和您的应用程序，如图1.4所示。

**图1.4您的应用程序以及其他应用程序正在通过统一的API与Spark的四个支柱（SQL，流技术，机器学习和图形）进行对话。 Spark使您免受操作系统和硬件限制：您不必担心应用程序在何处运行或它是否具有正确的数据。 Spark会解决这个问题。 但是，如果需要，您的应用程序仍然可以访问操作系统或硬件。**

![](https://drek4537l1klr.cloudfront.net/perrin/HighResolutionFigures/figure_1-4.png)

当然，运行Spark的群集可能不会被您的应用程序独占使用，但是您的工作将使用以下内容：
- Spark SQL 运行数据操作，如RDBMS中的传统SQL作业。 Spark SQL提供API和SQL来处理您的数据。 您将在第11章中发现Spark SQL，并在随后的大多数章节中阅读有关它的更多信息。 Spark SQL是Spark的基石。
- Spark Streaming（特别是Spark结构化的流）来分析流数据。 Spark的统一API将帮助您以类似的方式处理数据，无论是流数据还是批处理数据。 您将在第10章中了解有关流式传输的详细信息。
- 用于机器学习的Spark MLlib和深度学习的最新扩展。 机器学习，深度学习和人工智能值得一读。
- GraphX利用图数据结构。 要了解有关GraphX的更多信息，您可以阅读Michael Malak和Robin East（Manning，2016）编写的Spark GraphX in Action。


# 如何使用Spark
在本部分中，您将通过关注典型的数据处理方案以及数据科学方案来详细了解如何使用Apache Spark。 无论您是数据工程师还是数据科学家，您都可以在工作中使用Apache Spark。

## 数据处理/工程场景中的Spark

Spark可以多种方式处理您的数据。 但是，当在大数据场景中播放数据，清理，转换和重新发布数据时，它会表现出色。

我喜欢将数据工程师视为数据准备者和数据后勤人员。 他们确保数据可用，数据质量规则已成功应用，转换成功执行，并且数据可用于其他系统或部门，包括业务分析师和数据科学家。 数据工程师也可以是承担数据科学家工作并将其产业化的人。

Spark是数据工程师的理想工具。 数据工程执行的典型Spark（大数据）场景的四个步骤如下：
- 摄取
- 改善数据质量（DQ）
- 转型
- 出版物


图1.5典型数据处理场景中的Spark。 第一步是摄取数据。 在这个阶段，数据是原始的； 您可能接下来要应用一些数据质量（DQ）。 现在您可以转换数据了。 转换数据后，数据将变得更加丰富。 现在是发布或共享该文件的时候，以便组织中的人员可以对其执行操作并根据该文件做出决策。

![](https://drek4537l1klr.cloudfront.net/perrin/HighResolutionFigures/figure_1-5.png)

该过程包括四个步骤，并且在每个步骤之后，数据都会进入一个区域：

1. 摄取数据-Spark可以从各种来源摄取数据（请参阅有关摄取的第7、8和9章）。 如果找不到受支持的格式，则可以构建自己的数据源。 我现阶段将数据称为原始数据。 您还可以找到此区域，称为 staging, landing, bronze, swamp zone。
